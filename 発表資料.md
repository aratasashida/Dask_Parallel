# Machine spec to use experiment
```python
Python version  : 3.6.1
compiler        : GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)

system     : Darwin
release    : 16.7.0
machine    : x86_64
processor  : i386
CPU count  : 4
interpreter: 64bit
```

# What is Dask ??
Dask is a flexible parallel computing library for analytic computing using python.  
“Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of the dynamic task schedulers.  
 <img src=images/daskimage.png align=center>

## Familiar user interface
### example 1 : Pandas
```python
import pandas as pd  
df = pd.read_csv('2015-01-01.csv')  
df.groupby(df.user_id).value.mean()  

import dask.dataframe as dd  
df = dd.read_csv('2015-*-*.csv')  
df.groupby(df.user_id).value.mean().compute()  
```

### example 2 : numpy
```python
import numpy as np  
f = h5py.File('myfile.hdf5')  
x = np.array(f['/small-data'])  
x - x.mean(axis=1)

import dask.array as da  
f = h5py.File('myfile.hdf5')  
x = da.from_array(f['/big-data'],chunks=(1000, 1000))  
x - x.mean(axis=1).compute()  
```

## When you use dask ?
The Dask library was created assuming out-of-core.  
Therefore, if the processing fits in memory, use API of pandas or numpy, if you do not fit, use Dask.  
However, since it is not suitable for considerably large-scale calculation, in that case use a library such as spark.   

Use assumption memory size:  
numpy,pandas < dask < spark

# Simple parallel processing using Dask
A simple example of execution is shown below.  
Each function expresses addition, increment, sum respectively.  
```python
from time import sleep

def slowadd(x,y):
    """ addをスローで行う """
    sleep(1)
    return x+y

def slowinc(x):
    """ incrementをスローで行う """
    sleep(1)
    return x+1

def slowsum(L):
    """ sumをスローで行う """
    sleep(1)
    return sum(L)
```
Let's process with the function declared above.  
It is an easy process just to create a list and add contents.  
```python
%%time
​
data = [1,2,3]
A = [slowinc(i) for i in data]
B = [slowadd(a,10) for a in A]
C = [slowadd(b,100) for b in B]
score = slowsum(A) + slowsum(B) + slowsum(C)
print(score)
```
```python
387
CPU times: user 1.52 ms, sys: 1.41 ms, total: 2.93 ms
Wall time: 12 s
```

It takes about 12 seconds to execute.

## Parallelize using Dask
```python
from dask import delayed
%%time

data = [1,2,3]
A = [delayed(slowinc)(i) for i in data]
B = [delayed(slowadd)(b,10) for b in A]
C = [delayed(slowadd)(c,100) for c in B]
score = delayed(slowsum)(A) + delayed(slowsum)(B) + delayed(slowsum)(C)

print(score)
```
```python
Delayed('add-c46a2efa72a0682c848925b6e4d9b100')
CPU times: user 2.16 ms, sys: 1.08 ms, total: 3.24 ms
Wall time: 2.82 ms
```
Processing time seems to be shorter, but calculation is not done here.  
In this case, only parallel processing mapping is performed.  

```python
score.visualize()
```
 <img src=images/sum_list.png align=center>  
dask automatically performs parallel mapping.  
```python
%%time
score.compute()
```
```python
CPU times: user 9.87 ms, sys: 3.82 ms, total: 13.7 ms
Wall time: 4.02 s
```
The result of the processing is as follows.  
In dask, delay evaluation is realized by performing processing after mapping.   
Since Dask calculates each node in Computational Graph in order, it is unnecessary for all the data to be processed to be on the memory at the same time. Therefore, it is possible to handle data of a size exceeding the physical memory of the PC.  

### Comparison of processing time
```python
not using dask:
Wall time: 12 s
using dask:
Wall time: 4.02 s
```
It is faster when parallel processing is performed using dask.

# Mapping with a dictionary
In Dask, you can write parallel processing using dictionary type.  
```python
# 並列処理に使用する関数を定義しておく
def inc(x):
    return x+1

def add(x,y):
    return x+y

# daskでは辞書型を用いてグラフを表現する
dask_dict = {
    "a":1,
    "b":(inc,"a"),
    "x":2,
    "y":(inc,"x"),
    "z":(add,"b","y")
}
# 作成した辞書を表示
print(dask_dict)

# getメソッドを用いてz(加算結果)の値を取得する
from dask.multiprocessing import get
get(dask_dict,"z")
```
```python
{'a': 1,
 'b': (<function __main__.inc>, 'a'),
 'x': 10,
 'y': (<function __main__.inc>, 'x'),
 'z': (<function __main__.add>, 'b', 'y')}

5
```
The mapping result is as follows.  
<img src=images/add.png align=center>  
By using Dask like this, the user can easily implement parallel processing without being conscious.

# Speed ​​comparison by Monte Carlo method
below program is non parallelize Monte Carlo method.
```python
import random

def is_incide_circle():
    """ランダムな1点を作成し、指定した円の範囲内にあるかどうかを判定する"""
    x = random.random()
    y = random.random()
    if x**2 + y**2 <= 1:
        return 1
    else:
        return 0

def compute_pi(num):
    """引数回シュミレーションを行い、円周率の計算を行う"""
    count = [is_incide_circle() for i in range(num)]
    return 4*sum(count)/num

%%time
print(compute_pi(10000000))
```
```python
3.1416598
CPU times: user 1min 40s, sys: 4.22 s, total: 1min 44s
Wall time: 1min 52s
```
As a result of 100 million times simulation, the processing time was 1 min 52 s.  
The case of parallel mounting is as follows.  

```python
def how_many_inside_circle(k):
    return sum(is_incide_circle() for i in range(int(k)))

def parallel_compute2_pi(nsamples, k):
    points = [delayed(how_many_inside_circle)(k) for i in range(int(nsamples / k))]
    if nsamples % k != 0:
        points.append(delayed(how_many_inside_circle)(nsamples % k))
    return 4. * delayed(sum)(points) / nsamples

%%time
score = parallel_compute2_pi(100000000,1000)
print(score.compute())
```
```python
3.14140436
CPU times: user 2min 53s, sys: 7.69 s, total: 3min
Wall time: 3min 2s
```
<img src=images/montecarulo.png align=center>  
The processing time slowed down.It is because Dusk generates 1ms overhead during mapping. Although it is small enough to be ignore in the original calculation, in the case of a small process of hitting a point like the Monte Carlo method, calculation time is dominated by overhead.
